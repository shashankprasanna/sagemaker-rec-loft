{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Recommender System with Amazon SageMaker Factorization Machines and BlazingText\n",
    "\n",
    "---\n",
    "\n",
    "## Background\n",
    "\n",
    "- Recommender systems were a catalyst for ML's popularity (Amazon, Netflix Prize)\n",
    "- User item matrix factorization is a core methodology\n",
    "- Factorization machines combine linear prediction with a factorized representation of pairwise feature interaction\n",
    "\n",
    "$$\\hat{r} = w_0 + \\sum_{i} {w_i x_i} + \\sum_{i} {\\sum_{j > i} {\\langle v_i, v_j \\rangle x_i x_j}}$$\n",
    "\n",
    "- SageMaker has a highly scalable factorization machines algorithm built-in\n",
    "- To learn more about the math behind _factorization machines_, [this paper](https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf) is a great resource\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Setup (DONE)\n",
    "\n",
    "### Follow instructions on-screen\n",
    "\n",
    "1. Spin up SageMaker hosted notebook instance in console\n",
    "2. Add SageMaker IAM policy to this SageMaker notebook to allow S3 read/write access\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Data preparation\n",
    "\n",
    "1. Create new S3 bucket (first cell)\n",
    "2. Import necessary libraries (second cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "base = 'DEMO-loft-recommender'\n",
    "prefix = 'sagemaker/' + base\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import boto3\n",
    "import json\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "import sagemaker.amazon.common as smac\n",
    "from sagemaker.predictor import json_deserializer\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import dataset\n",
    "[Amazon Reviews AWS Public Dataset](https://s3.amazonaws.com/amazon-reviews-pds/readme.html)\n",
    "- 1 to 5 star ratings\n",
    "- 2M+ Amazon customers\n",
    "- 160K+ digital videos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir /tmp/recsys/\n",
    "!aws s3 cp s3://amazon-reviews-pds/tsv/amazon_reviews_us_Digital_Video_Download_v1_00.tsv.gz /tmp/recsys/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/tmp/recsys/amazon_reviews_us_Digital_Video_Download_v1_00.tsv.gz', delimiter='\\t',error_bad_lines=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset columns:\n",
    "\n",
    "- `marketplace`: 2-letter country code (in this case all \"US\").\n",
    "- `customer_id`: Random identifier that can be used to aggregate reviews written by a single author.\n",
    "- `review_id`: A unique ID for the review.\n",
    "- `product_id`: The Amazon Standard Identification Number (ASIN).  `http://www.amazon.com/dp/<ASIN>` links to the product's detail page.\n",
    "- `product_parent`: The parent of that ASIN.  Multiple ASINs (color or format variations of the same product) can roll up into a single parent parent.\n",
    "- `product_title`: Title description of the product.\n",
    "- `product_category`: Broad product category that can be used to group reviews (in this case digital videos).\n",
    "- `star_rating`: The review's rating (1 to 5 stars).\n",
    "- `helpful_votes`: Number of helpful votes for the review.\n",
    "- `total_votes`: Number of total votes the review received.\n",
    "- `vine`: Was the review written as part of the [Vine](https://www.amazon.com/gp/vine/help) program?\n",
    "- `verified_purchase`: Was the review from a verified purchase?\n",
    "- `review_headline`: The title of the review itself.\n",
    "- `review_body`: The text of the review.\n",
    "- `review_date`: The date the review was written.\n",
    "\n",
    "Drop some fields that won't be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['customer_id', 'product_id', 'product_title', 'star_rating', 'review_date']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop some fields that won't be used for now. We'll reintroduce these later in a format that is expected by the algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers = df['customer_id'].value_counts()\n",
    "products = df['product_id'].value_counts()\n",
    "\n",
    "quantiles = [0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.96, 0.97, 0.98, 0.99, 1]\n",
    "print('customers\\n', customers.quantile(quantiles))\n",
    "print('products\\n', products.quantile(quantiles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter out customers who haven't rated many movies.\n",
    "\n",
    "Only keep customers who've rated more than or equal to 5 products\n",
    "And products which have more than or equal to 10 ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers = customers[customers >= 5]\n",
    "products = products[products >= 10]\n",
    "\n",
    "reduced_df = df.merge(pd.DataFrame({'customer_id': customers.index})).merge(pd.DataFrame({'product_id': products.index}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a sequential index for customers and movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers = reduced_df['customer_id'].value_counts()\n",
    "products = reduced_df['product_id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_index = pd.DataFrame({'customer_id': customers.index, 'user': np.arange(customers.shape[0])})\n",
    "product_index = pd.DataFrame({'product_id': products.index, \n",
    "                              'item': np.arange(products.shape[0]) + customer_index.shape[0]})\n",
    "\n",
    "reduced_df = reduced_df.merge(customer_index).merge(product_index)\n",
    "reduced_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count days since first review (included as a feature to capture trend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_df['review_date'] = pd.to_datetime(reduced_df['review_date'])\n",
    "customer_first_date = reduced_df.groupby('customer_id')['review_date'].min().reset_index()\n",
    "customer_first_date.columns = ['customer_id', 'first_review_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_df = reduced_df.merge(customer_first_date)\n",
    "reduced_df['days_since_first'] = (reduced_df['review_date'] - reduced_df['first_review_date']).dt.days\n",
    "reduced_df['days_since_first'] = reduced_df['days_since_first'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = reduced_df.groupby('customer_id').last().reset_index()\n",
    "\n",
    "train_df = reduced_df.merge(test_df[['customer_id', 'product_id']], \n",
    "                            on=['customer_id', 'product_id'], \n",
    "                            how='outer', \n",
    "                            indicator=True)\n",
    "train_df = train_df[(train_df['_merge'] == 'left_only')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Factorization machines expects data to look something like:\n",
    "  - Sparse matrix\n",
    "  - Target variable is that user's rating for a movie\n",
    "  - One-hot encoding for users ($N$ features)\n",
    "  - One-hot encoding for movies ($M$ features)\n",
    "\n",
    "|Rating|User1|User2|...|UserN|Movie1|Movie2|Movie3|...|MovieM|Feature1|Feature2|...|\n",
    "|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
    "|4|1|0|...|0|1|0|0|...|0|20|2.2|...|\n",
    "|5|1|0|...|0|0|1|0|...|0|17|9.1|...|\n",
    "|3|0|1|...|0|1|0|0|...|0|3|11.0|...|\n",
    "|4|0|1|...|0|0|0|1|...|0|15|6.4|...|\n",
    "\n",
    "\n",
    "- Wouldn't want to hold this full matrix in memory\n",
    "  - Create a sparse matrix\n",
    "  - Designed to work efficiently with CPUs. Some parts of training for more dense matrices can be parallelized with GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_csr_matrix(df, num_users, num_items):\n",
    "    feature_dim = num_users + num_items + 1\n",
    "    data = np.concatenate([np.array([1] * df.shape[0]),\n",
    "                           np.array([1] * df.shape[0]),\n",
    "                           df['days_since_first'].values])\n",
    "    row = np.concatenate([np.arange(df.shape[0])] * 3)\n",
    "    col = np.concatenate([df['user'].values,\n",
    "                          df['item'].values,\n",
    "                          np.array([feature_dim - 1] * df.shape[0])])\n",
    "    return csr_matrix((data, (row, col)), \n",
    "                      shape=(df.shape[0], feature_dim), \n",
    "                      dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csr = to_csr_matrix(train_df, customer_index.shape[0], product_index.shape[0])\n",
    "test_csr = to_csr_matrix(test_df, customer_index.shape[0], product_index.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to sparse recordIO-wrapped protobuf that SageMaker factorization machines expects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_s3_protobuf(csr, label, bucket, prefix, channel='train', splits=10):\n",
    "    indices = np.array_split(np.arange(csr.shape[0]), splits)\n",
    "    for i in range(len(indices)):\n",
    "        index = indices[i]\n",
    "        buf = io.BytesIO()\n",
    "        smac.write_spmatrix_to_sparse_tensor(buf, csr[index, ], label[index])\n",
    "        buf.seek(0)\n",
    "        boto3.client('s3').upload_fileobj(buf, bucket, '{}/{}/data-{}'.format(prefix, channel, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_s3_protobuf(train_csr, train_df['star_rating'].values.astype(np.float32), bucket, prefix)\n",
    "to_s3_protobuf(test_csr, test_df['star_rating'].values.astype(np.float32), bucket, prefix, channel='test', splits=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3 - Train Factorization Machines (FM) using SageMaker\n",
    "\n",
    "- Create a [SageMaker Python SDK](https://github.com/aws/sagemaker-python-sdk) estimator to run a training jobs and specify:\n",
    "  - Algorithm container image\n",
    "  - IAM role\n",
    "  - Hardware setup\n",
    "  - S3 output location\n",
    "  - Algorithm hyperparameters\n",
    "    - `feature_dim`: $N + M + 1$ (additional feature is `days_since_first` to capture trend)\n",
    "    - `num_factors`: number of factor dimensions (increasing too much can lead to overfitting)\n",
    "    - `epochs`: number of full passes through the dataset\n",
    "- `.fit()` points to training and test data in S3 and begins the training job\n",
    "\n",
    "**Note**: For AWS accounts registered in conjunction with a workshop, default instance limits may prevent the use of `ml.c5.2xlarge` (and other equally powerful instances), and may require a lower value for `train_instance_count` depending on the instance type chosen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fm = sagemaker.estimator.Estimator(\n",
    "    sagemaker.amazon.amazon_estimator.get_image_uri(boto3.Session().region_name, 'factorization-machines', 'latest'),\n",
    "    role, \n",
    "    train_instance_count=4, \n",
    "    train_instance_type='ml.c5.2xlarge',\n",
    "    output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "    base_job_name=base,\n",
    "    sagemaker_session=sess)\n",
    "\n",
    "fm.set_hyperparameters(\n",
    "    feature_dim=customer_index.shape[0] + product_index.shape[0] + 1,\n",
    "    predictor_type='regressor',\n",
    "    mini_batch_size=1000,\n",
    "    num_factors=256,\n",
    "    epochs=3)\n",
    "\n",
    "fm.fit({'train': sagemaker.s3_input('s3://{}/{}/train/'.format(bucket, prefix), distribution='ShardedByS3Key'), \n",
    "        'test': sagemaker.s3_input('s3://{}/{}/test/'.format(bucket, prefix), distribution='FullyReplicated')})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4 - Host the trained model\n",
    "\n",
    "Deploy trained model to a real-time production endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fm_predictor = fm.deploy(instance_type='ml.m4.xlarge', initial_instance_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup predictor to serialize in-memory data for invocation requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fm_serializer(df):\n",
    "    feature_dim = customer_index.shape[0] + product_index.shape[0] + 1\n",
    "    js = {'instances': []}\n",
    "    for index, data in df.iterrows():\n",
    "        js['instances'].append({'data': {'features': {'values': [1, 1, data['days_since_first']],\n",
    "                                                      'keys': [data['user'], data['item'], feature_dim - 1],\n",
    "                                                      'shape': [feature_dim]}}})\n",
    "    return json.dumps(js)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_predictor.content_type = 'application/json'\n",
    "fm_predictor.serializer = fm_serializer\n",
    "fm_predictor.deserializer = json_deserializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Real-time prediction for what a single user would rate an item**\n",
    "\n",
    "1. Pick a customer-movie pair from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Pull out a single customer-movie pair that we like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_customer = test_df.iloc[[20]]\n",
    "test_df.iloc[[20]] # peek at the data to confirm it's the one we wanted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Pass `test_customer` to predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fm_predictor.predict(test_customer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now let's make a df for an arbitrary customer and movie pair and test it out!**\n",
    "\n",
    "Our `fm_serializer` requires 3 inputs to perform a prediction:\n",
    " - `user` id for a customer (type = num)\n",
    " - `item` id for a movie (type = num)\n",
    " - `days_since_first` review (type = double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fake_customer = test_customer # make a copy of the test_customer we pulled out before to modify\n",
    "desired_user_id = 65884 # person who rated Dexter with 5 stars\n",
    "desired_item_id = 140461 # Code for True Blood: Season 1\n",
    "desired_review_days = 10.0 # arbitrary number of days since first review\n",
    "\n",
    "#fake_customer_data = {'user' : desired_user_id, 'item' : desired_item_id, 'days_since_first' : desired_review_days}\n",
    "#fake_customer = pd.DataFrame(fake_customer_data, index=[0])\n",
    "fake_customer['user'] = desired_user_id\n",
    "fake_customer['item'] = desired_item_id\n",
    "fake_customer['days_since_first'] = desired_review_days\n",
    "\n",
    "# print the details for this fake customer\n",
    "fake_customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_predictor.predict(fake_customer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final step: Clean-up the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finished?\n",
    "\n",
    "Got some extra time? Feel free to go on to the Extra Credit below! \n",
    "\n",
    "**Note**: Amazon SageMaker automatically handles provisioning and tearing down of resources during training. Once deployed, the model's endpoint will persist independent of this notebook, and can be removed with the cell directly above this. \n",
    "\n",
    "If you are done working with this notebook demo, it is strongly advised that you stop the SageMaker hosted notebook instance if you do not wish to continue using it (and incurring costs). This can easily be done by clicking on \"Notebook instances\" from the SageMaker console."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Extra credit \n",
    "## (not covered in today's lab)\n",
    "\n",
    "- What happens when a new movie is added?\n",
    "  - No feature to set to \"1\" in the dataset\n",
    "  - No previous ratings to find similar items\n",
    "  - Cold start problem is hard with factorization machines\n",
    "- Word2vec\n",
    "  - Word embeddings for natural language processing (similar words get similar vectors)\n",
    "  - Use concatenated product titles as words, customer review history as sentences\n",
    "  - SageMaker BlazingText is an extremely fast implementation that can work with subwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Data\n",
    "\n",
    "Concatenate product titles to treat each one as a single word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_df['product_title'] = reduced_df['product_title'].apply(lambda x: x.lower().replace(' ', '-'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write customer purchase histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first = True\n",
    "with open('customer_purchases.txt', 'w') as f:\n",
    "    for customer, data in reduced_df.sort_values(['customer_id', 'review_date']).groupby('customer_id'):\n",
    "        if first:\n",
    "            first = False\n",
    "        else:\n",
    "            f.write('\\n')\n",
    "        f.write(' '.join(data['product_title'].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write to S3 so SageMaker training can use it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = sess.upload_data('customer_purchases.txt', bucket, '{}/word2vec/train'.format(prefix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Train\n",
    "\n",
    "Create a SageMaker estimator:\n",
    "- Specify training job arguments\n",
    "- Set hyperparameters\n",
    "  - Remove titles that occur less than 5 times\n",
    "  - Embed in a 100-dimensional subspace\n",
    "  - Use subwords to capture similarity in titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bt = sagemaker.estimator.Estimator(\n",
    "    sagemaker.amazon.amazon_estimator.get_image_uri(boto3.Session().region_name, 'blazingtext', 'latest'),\n",
    "    role, \n",
    "    train_instance_count=1, \n",
    "    train_instance_type='ml.p3.2xlarge',\n",
    "    train_volume_size = 5,\n",
    "    output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "    sagemaker_session=sess)\n",
    "\n",
    "bt.set_hyperparameters(mode=\"skipgram\",\n",
    "    epochs=10,\n",
    "    min_count=5,\n",
    "    sampling_threshold=0.0001,\n",
    "    learning_rate=0.05,\n",
    "    window_size=5,\n",
    "    vector_dim=100,\n",
    "    negative_samples=5,\n",
    "    min_char=5,\n",
    "    max_char=10,\n",
    "    evaluation=False,\n",
    "    subwords=True)\n",
    "\n",
    "bt.fit({'train': sagemaker.s3_input(inputs, distribution='FullyReplicated', content_type='text/plain')})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Model\n",
    "\n",
    "- Bring in and extract the model from S3\n",
    "- Take a look at the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp $bt.model_data ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xvzf model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = pd.read_csv('vectors.txt', delimiter=' ', skiprows=2, header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the embeddings appear to have meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors.sort_values(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors.sort_values(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, but there's 100.  Let's reduce this further with t-SNE and map the top 100 titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_titles = vectors[0]\n",
    "vectors = vectors.drop([0, 101], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(perplexity=40, n_components=2, init='pca', n_iter=10000)\n",
    "embeddings = tsne.fit_transform(vectors.values[:100, ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pylab\n",
    "%matplotlib inline\n",
    "\n",
    "def plot(embeddings, labels):\n",
    "    pylab.figure(figsize=(20,20))\n",
    "    for i, label in enumerate(labels):\n",
    "        x, y = embeddings[i,:]\n",
    "        pylab.scatter(x, y)\n",
    "        pylab.annotate(label, xy=(x, y), xytext=(5, 2), textcoords='offset points',\n",
    "                       ha='right', va='bottom')\n",
    "    pylab.show()\n",
    "\n",
    "plot(embeddings, product_titles[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Host\n",
    "\n",
    "Deploy our model to a real-time endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bt_endpoint = bt.deploy(initial_instance_count = 1,instance_type = 'ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try generating predictions for a set of titles (some of which are real, some of which are made up)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"sherlock-season-1\", \n",
    "         \"sherlock-season-2\",\n",
    "         \"sherlock-season-5\",\n",
    "         'arbitrary-sherlock-holmes-string',\n",
    "         'the-imitation-game',\n",
    "         \"abcdefghijklmn\",\n",
    "         \"keeping-up-with-the-kardashians-season-1\"]\n",
    "\n",
    "payload = {\"instances\" : words}\n",
    "\n",
    "response = bt_endpoint.predict(json.dumps(payload))\n",
    "\n",
    "vecs_df = pd.DataFrame(json.loads(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate correlation and distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs_df = pd.DataFrame(vecs_df['vector'].values.tolist(), index=vecs_df['word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs_df = vecs_df.transpose()\n",
    "vecs_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in vecs_df.columns:\n",
    "    print(column + ':', np.sum((vecs_df[column] - vecs_df['sherlock-season-1']) ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relative to 'sherlock-season-1':\n",
    "- 'sherlock-season-5' is made up, but relates well with 'sherlock-season-1' and 'sherlock-season-2'\n",
    "- 'arbitrary-sherlock-holmes-string' is also made up and relates less well but still fairly strong\n",
    "- 'the-imitation-game' is another popular Prime video title starring Benedict Cumberbatch and has a moderate relationship, but worse than the arbitrary Sherlock title\n",
    "- 'abcdefghijklmn' is made up and relates even worse\n",
    "- 'keeping-up-with-the-kardashians-season-1' somehow manages to relate even worse\n",
    "\n",
    "Clean-up the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bt_endpoint.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---\n",
    "\n",
    "# Wrap-up\n",
    "\n",
    "- Built a recommender system on a large dataset quickly and accurately\n",
    "- Add more features to extend\n",
    "- Compare to other methods\n",
    "- Ensemble two models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
